<!DOCTYPE html>
<html lang="en">
<head>
  <!-- project.html — Machine Learning Project 3 -->
  <!-- HTML comments included per rubric -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Tech Heroes • LIS500</title>
  <link rel="stylesheet" href="stylepage.css">
</head>
<body>
  <a class="skip-link" href="#main">Skip to content</a>

  <header>
    <nav aria-label="Primary">
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="about-us.html">About Us</a></li>
        <li><a href="resources.html">Resources</a></li>
        <li><a class="active" href="tech-heroes.html">Tech Heroes</a></li>
        <li><a href="project.html">Machine Learning</a></li>
      </ul>
    </nav>
  </header>

  <main id="main" class="container">
    <section class="section">
    <!-- HTML Code For Project -->
      <h1>Image Classifier (Teachable Machine)</h1>
      <p class="lead">This page uses our Teachable Machine image model to classify objects in real time using your webcam.</p>
      <p>All you have to do is just click the button below to start your camera, and then hold one of the trained objects in front of the webcam to see the live prediction.</p>

      <button type="button" onclick="init()">Camera</button>

      <div id="webcam-container"></div>
      <div id="label-container"></div>
    </section>
    
    <!-- Code For Video Demo -->

    <section class="section">
      <h2>Video Demonstration</h2>
      <div style="text-align:center;">
        <iframe width="560" height="315"
          src="https://www.youtube.com/embed/lPQ-w_gGXOE"
          title="YouTube Algorithm Demo"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen>
        </iframe>
      </div>
      <p>This short video demonstrates our Teachable Machine model classifying objects in real time.</p>
    </section>

    <section class="section">
      <h2>Project Statement</h2>
 
      <p>One of the most important ideas we focused on in our project was Buolamwini’s thoughts about implicit bias and intersectionality. Unmasking AI showed us how bias in machine learning doesn’t necessarily appear randomly, but rather it is because of a bias. More specifically, people who are at the intersection of multiple marginalized identities, such as darker-skinned women, experience the highest error rates because the data reflects existing social inequalities. This helped us understand that bias in machine learning doesn’t affect everyone equally, but instead affects people of color more, and that is something that needs to seriously be fixed. These failures in the systems aren't usually a result of bad intent, but more so because of the implicit bias embedded in the datasets, as well as the assumptions that the engineers bring while developing the code. Intersectionality also helped us understand that the issues with AI isn’t really isolated, since it usually shows up more for people who belong to more than one marginalized group at once. Buolamwini highlighted how darker skinned women weren’t just misclassified due to their gender or race alone, but because the combination of those two identities places them outside of the system’s narrow training distribution and dataset. This layered vulnerability makes intersectional bias especially harmful and impactful, since multiple identities can end up being ignored while errors increase. Understanding this issue made us act more carefully when creating our model. It also showed us how even simple models had the ability to mirror structural inequality if they aren’t designed carefully. This helped us understand that fairness requires intentionally including what the world sometimes overlooks, and to be more considerate about who the dataset can affect if bias is present.</p>

      <p>Ellen Pao’s “Tech, Heal Thyself” also shaped how we approached our project. Pao heavily critiques Silicon Valley’s culture of meritocracy and how power is constantly held by male dominated groups. She describes how Silicon Valley celebrates “fairness” in theory, but in practice usually only rewards the same group of white men who already hold power. This reading highlights that power and bias doesn’t just randomly appear, but rather it is definitively reflected in the social structures and biases of those who create and run these systems, which heavily mirrors how models and datasets can also reproduce existing inequalities. When the people who hold power decide what gets built, how it is built, and whose data is collected, the systems tend to cater toward those specific groups. Pao’s argument taught us that simply adding a few diverse examples won’t fix a biased process, just like how hiring a few women won’t actually change a hostile workplace environment. We applied this lesson to our project by treating dataset design as more of a cultural question, as opposed to just a technical one. We didn’t just only add different colors, but we also made sure to intentionally vary the angles, background, lighting, and environment so that the dataset would reflect a larger set of conditions. Just like Silicon Valley’s version of meritocracy can reinforce existing power structures, AI systems can reinforce social inequalities if developers aren’t aware of who is being included or excluded while they create their systems.</p>

      <p>Although our project focuses on classifying colored objects instead of human faces, we still used Buolamwini’s lessons to shape our approach. We intentionally chose to build a very simple image classifier, with it being one that only distinguishes between colors. This simplicity was deliberate, since by avoiding human faces and any other sensitive categories, we reduced the possibility of reproducing demographic harm while still being able to examine the core ideas of machine learning bias. For our dataset, we collected around sixty images of different colored objects and made sure to vary the lighting, angles, backgrounds, and distance from the camera, since doing this would help us minimize environmental and dataset bias exponentially. Our goal was for the model to only distinguish objects by color, as this specified focus on color and not on features like racial, cultural, or human ones would help to avoid the bias issue Joy Buolamwini discussed in her book. This also ensured that our model had no harmful impacts, since it wasn’t classifying people at all. By diversifying our dataset, we prevented the model from learning that there is only one “correct” environment, a major flaw in the factional recognition systems trained under idealized and unrepresentative conditions that Buolamwini heavily critiqued. Even with a model as simple as ours, we began to see how easily structural issues like environmental bias, dataset imbalance, and gaps in representation can appear. This helped us understand that bias isn’t just a social issue, and it can also be a technical one that emerges from even small design decisions, reinforcing the importance of intentional and ethical dataset creation.</p>

      <p>The reason we chose a non-human classification task was because, in Unmasking AI, Buolamwini described how facial recognition systems consistently failed to detect darker-skinned women due to training datasets made up mostly of lighter-skinned male faces. She talked about how this made her feel erased by the system, which clearly demonstrated how harmful exclusionary algorithmic design can be. Her experiences made us think more carefully about what kind of model we should use for this project. If we built a human based classifier that could potentially identify poses or faces, we could easily run into ethical issues. As Buolamwini highlights, these systems carry a history of racial bias and unequal performance across identities.</p>
      
      <p>We wanted to create an unbiased model that could account for different environmental conditions. By using colored objects instead of human features, we were able to focus entirely on the technical aspects of data without risking harmful patterns related to gender, skin tone, race, or appearance. This choice also aligns with one of the main lessons from Unmasking AI: we must ask not only what AI can do, but what it should do. Building a low-risk and safe model was the responsible choice. Bias could still occur if our dataset lacked variation, but we addressed this by intentionally changing the aforementioned variables such as lighting, angles, and backgrounds to minimize potential issues.</p>

      <p>Furthermore, although our model focuses on non-human objects, we still wanted to approach our dataset with an awareness of bias and its implications. In Unmasking AI, Buolamwini explains that bias enters systems based on how certain people are represented, who is represented, and who is left out completely. Her example from the MIT Gender Shades study showed how commercial models were more accurate for light skinned men but performed poorly for dark skinned women because the dataset was extremely unbalanced. To apply these lessons to our own work, we made sure to vary our images. This prevented the model from overfitting into one ideal or narrow condition. The systems that Buolamwini focused her studies in performed poorly on darker skinned faces primarily because they were trained under ideal lighting and conditions which don't reflect any real-world conditions. By diversifying our images, data, and environmental conditions, we were able to reduce this type of environmental bias.</p>

      <p>This part of the project helped us understand that bias is not only about who is included in a dataset, but also how the data is collected, structured, and organized. Even with our simple model, bias could have appeared easily if we had only taken images of one color under studio lighting. In that case, the model would have struggled to recognize the same color in different lighting or settings. We were intentional about avoiding this issue, and the aforementioned diversification of our dataset helped ensure a more fair and reliable outcome.</p>

      <p>For the model training process, we used Teachable Machine’s image classification tool. This tool uses a MobileNet architecture to extract features from the images. After uploading our dataset in the form of images and data, we trained the model using three objects: a pair of red scissors, a black sticky spray bottle, and a blue sticky note. These objects served as the examples for the three color categories. Once the model finished training, we directly exported it to TensorFlow.js so it could run directly in the browser on our project website.</p> 

      <p>We quickly noticed that the more images we added, the better the model performed. It became more accurate at recognizing each color, even when we used entirely different objects that it had not seen before. This made it clear that dataset diversity plays an important role in an algorithm’s fairness and reliability. What surprised us most during testing was how well the model generalized its learning. For instance, when we showed it a red notebook, it correctly associated the color with the red scissors it had been trained on. Seeing the model successfully make these connections was quite intriguing because it felt like a small-scale version of how real-world machine learning systems categorize and interpret information today.</p>
  
      <p>Some of the key lessons from Unmasking AI were essential to shaping how we approached this project. One of the most important lessons was that bias is a design choice rather than an accident. Buolamwini makes it clear that biased systems are built through human decisions, rather than simply appearing on their own. As we collected our data, we became more aware of how every decision we made, including lighting, angles, object choice, and environment, could influence how the model behaved. Fair and unbiased design requires intentional thought, and won’t exist if based on guesswork.</p>

      <p>A second key lesson we took away was that representation matters. The lack of representation in facial recognition datasets harmed real people, and we wanted to make sure our project avoided that pattern entirely. Even though our project did not involve human subjects, we still applied the principle of balanced representation across all object categories and environmental conditions.</p>

      <p>Another major lesson was that fairness in models requires questioning a system’s purpose. Buolamwini emphasizes the importance of understanding who benefits from AI and who might be harmed. This guided our decision to use an object-based classifier so that we could avoid creating a model with any potential real-world negative impacts. The project made it clear that machine learning is not just a technical process of plugging in data and code, but instead one that involves ethics, responsibility, and awareness of consequences.</p>

      <p>Unmasking AI encouraged us to think critically about how datasets are created and who could be affected by biased data. The book showed how even small design choices can lead to harmful outcomes. By building a low-bias classifier in a controlled environment, we were able to practice these lessons in a hands-on way and gain a deeper understanding of how bias operates in AI systems. Although our model was simple, the process embodied one of Buolamwini’s most important points, this being that fairness in AI must be intentional. Bias does not revolve itself, and instead must be confronted with every step, beginning with the data and continuing through the code, the design decisions, and the overall purpose of the system.</p>

    </section>
    
  </main>

  <footer>
    <nav class="footer-nav" aria-label="Footer">
      <a href="#">UW–Madison</a>
      <a href="index.html">LIS500 Project #3</a>
      <a href="https://github.com/jasonthronson/lis500-project3" target="_blank">GitHub</a>
    </nav>
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4"></script>
  <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@0.8.4/dist/teachablemachine-image.min.js"></script>

  <script>
  <!-- Main Script Below -->

  const URL = "project/";

  let model, webcam, labelContainer;

  async function init() {
    const modelURL = URL + "model.json";
    const metadataURL = URL + "metadata.json";

    model = await tmImage.load(modelURL, metadataURL);
    maxPredictions = model.getTotalClasses();

    const flip = true;
    webcam = new tmImage.Webcam(300, 300, flip);
    await webcam.setup();
    await webcam.play();
    window.requestAnimationFrame(loop);

    document.getElementById("webcam-container").innerHTML = "";
    document.getElementById("webcam-container").appendChild(webcam.canvas);

    labelContainer = document.getElementById("label-container");
    labelContainer.innerHTML = "";
    for (let i = 0; i < maxPredictions; i++) {
      const div = document.createElement("div");
      labelContainer.appendChild(div);
    }
  }

  async function loop() {
    webcam.update();
    await predict();
    window.requestAnimationFrame(loop);
  }

  async function predict() {
    const prediction = await model.predict(webcam.canvas);
    for (let i = 0; i < maxPredictions; i++) {
      const className = prediction[i].className;
      const probability = (prediction[i].probability * 100).toFixed(1);
      labelContainer.childNodes[i].innerHTML = `${className}: ${probability}%`;
    }
  }

  </script>

</body>
</html>
